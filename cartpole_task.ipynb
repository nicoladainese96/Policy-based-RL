{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving cartpole task with policy-based RL algorithms\n",
    "\n",
    "**Decription of the task:**\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gym\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(agent, env, return_states=False):\n",
    "    # Reset environment (start of an episode)\n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    done = []\n",
    "    \n",
    "    if return_states:\n",
    "        states = [state]\n",
    "        \n",
    "        \n",
    "    steps = 0\n",
    "    while True:\n",
    "        action, log_prob = agent.get_action(state, return_log = True)\n",
    "        new_state, reward, terminal, info = env.step(action) # gym standard step's output\n",
    "        \n",
    "        if return_states:\n",
    "            states.append(new_state)\n",
    "            \n",
    "        if terminal and 'TimeLimit.truncated' not in info:\n",
    "            # give -1 if cartpole falls but not if episode is truncated\n",
    "            reward = -1 \n",
    "            \n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        done.append(terminal)\n",
    "        \n",
    "        if terminal:\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "       \n",
    "    rewards = np.array(rewards)\n",
    "    done = np.array(done)\n",
    "    \n",
    "    if return_states:\n",
    "        return rewards, log_probs, np.array(states), done\n",
    "    else:\n",
    "        return rewards, log_probs, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_test_episode(agent):\n",
    "    # Create environment\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = agent.get_action(state, return_log = False)\n",
    "        new_state, reward, terminal, info = env.step(action) # gym standard step's output\n",
    "        if terminal: \n",
    "            break\n",
    "        else: \n",
    "            state = new_state\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Policy-Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PolicyGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(PolicyGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_cartpole(n_episodes = 100, lr = 0.01, gamma = 0.99):\n",
    "    # Create environment\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    # Init agent\n",
    "    agent = PolicyGradient.PolicyGrad(observation_space, action_space, lr, gamma)\n",
    "    performance = []\n",
    "    losses = []\n",
    "    for e in range(n_episodes):\n",
    "        rewards, log_probs, _ = play_episode(agent, env)\n",
    "        performance.append(np.sum(rewards))\n",
    "        if (e+1)%10 == 0:\n",
    "            print(\"Episode %d - reward: %.0f\"%(e+1, np.mean(performance[-10:])))\n",
    "        \n",
    "        loss = agent.update(rewards, log_probs)\n",
    "        losses.append(loss)\n",
    "    return agent, np.array(performance), np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trained_agentPG, cumulative_rewardPG, lossesPG = train_cartpole(n_episodes = 500, lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = False\n",
    "if T:\n",
    "    n_runs = 30\n",
    "    results_v0 = []\n",
    "    for i in range(n_runs):\n",
    "        trained_agentPG, cumulative_rewardPG, lossesPG = train_cartpole(n_episodes = 500, lr=5e-3)\n",
    "        results_v0.append(cumulative_rewardPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if T:\n",
    "    np.save('Results/REINFORCE_perf', results_v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1,len(cumulative_rewardPG)+1)\n",
    "plt.plot(episodes, cumulative_rewardPG)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episodes, lossesPG)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_test_episode(trained_agentPG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor-Critic - trajectory version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ActorCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(ActorCritic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cartpole_A2C(n_epochs = 100, lr = 0.01, gamma = 0.99, TD=True, twin=False, tau=1.,**kwargs):\n",
    "    # Create environment\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    # Init agent\n",
    "    agent = ActorCritic.A2C(observation_space, action_space, lr, gamma, \n",
    "                            TD=TD, discrete=False, twin=twin, tau=tau, **kwargs)\n",
    "    performance = []\n",
    "    score = []\n",
    "    for e in range(n_epochs):\n",
    "        rewards, log_probs, states, done = play_episode(agent, env, return_states=True)\n",
    "        performance.append(np.sum(rewards))\n",
    "        if (e+1)%10 == 0:\n",
    "            print(\"Episode %d - reward: %.0f\"%(e+1, np.mean(performance[-10:])))\n",
    "        #print(\"rewards.shape \", rewards.shape)\n",
    "        #print(\"log_probs \", log_probs)\n",
    "        #print(\"states.shape \", states.shape)\n",
    "        #print(\"done.shape \", done.shape)\n",
    "        #print(\"done \", done)\n",
    "        agent.update(rewards, log_probs, np.array([states]) , done)\n",
    "        \n",
    "    return agent, np.array(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "HPs = dict(n_epochs=5000, lr=1e-3, twin=True,tau=0.1, debug=True, hiddens=[64,32,16], n_steps=3)\n",
    "agent_TD, performance_TD = train_cartpole_A2C(**HPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1,len(performance_TD)+1)\n",
    "plt.scatter(episodes, performance_TD, s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_test_episode(agent_TD) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "agent_MC, performance_MC = train_cartpole_A2C(n_epochs=1500, lr=5e-3, TD=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1,len(performance_MC)+1)\n",
    "plt.scatter(episodes, performance_MC, s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_test_episode(agent_MC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward shaping\n",
    "\n",
    "Try to make a more informative reward.\n",
    "Idea: store the whole trajectory, then subtract $-\\frac{eps \\cdot t}{T}$ to all rewards, where $t$ is the step at which the reward was obtained and $T$ the total number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_rewards(r, eps, power=1):\n",
    "    T = len(r)\n",
    "    t = np.arange(1,T+1)\n",
    "    r -= eps*(t/T)**power\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cartpole_A2C_shaped(n_epochs = 100, n_batches = 1, lr = 0.01, gamma = 0.99, TD=True, eps=1, power=1):\n",
    "    # Create environment\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    # Init agent\n",
    "    agent = ActorCritic.A2C(observation_space, action_space, lr, gamma, TD=TD, discrete=False)\n",
    "    performance = []\n",
    "    for e in range(n_epochs):\n",
    "        r_list = []\n",
    "        logp_list = []\n",
    "        s_list = []\n",
    "        done_list = []\n",
    "        score = []\n",
    "        \n",
    "        for b in range(n_batches):\n",
    "            rewards, log_probs, states, done = play_episode(agent, env, return_states=True)\n",
    "            if done[-1] == True and len(done) != 500:\n",
    "                rewards = shape_rewards(rewards, eps, power)\n",
    "            r_list.append(rewards)\n",
    "            logp_list.append(log_probs)\n",
    "            s_list.append(states)\n",
    "            done_list.append(done)\n",
    "            score.append(np.sum(rewards))\n",
    "            \n",
    "        performance.append(np.mean(score))\n",
    "        if (e+1)%10 == 0:\n",
    "            print(\"Episode %d - reward: %.0f\"%(e+1, np.mean(performance[-10:])))\n",
    "        exp_buff = experience_buffer(r_list, logp_list, s_list, done_list)\n",
    "        rewards, log_probs, states, done = exp_buff.get_exp()\n",
    "        #print(\"rewards.shape \", rewards.shape)\n",
    "        #print(\"log_probs \", log_probs)\n",
    "        #print(\"states.shape \", states.shape)\n",
    "        #print(\"done.shape \", done.shape)\n",
    "        #print(\"done \", done)\n",
    "        agent.update(rewards, log_probs[0], states, done)\n",
    "        \n",
    "    return agent, np.array(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "agent_TD_sh, performance_TD_sh = train_cartpole_A2C_shaped(n_epochs=1500, lr=5e-3, power=2, eps=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1,len(performance_TD_sh)+1)\n",
    "plt.scatter(episodes, performance_TD_sh, s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_test_episode(agent_TD_sh) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "agent_MC_sh, performance_MC_sh = train_cartpole_A2C_shaped(n_epochs=1500, lr=5e-3, power=2, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1,len(performance_MC_sh)+1)\n",
    "plt.scatter(episodes, performance_MC_sh, s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_test_episode(agent_MC_sh) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final discussion:** It is undisputable that A2C setup is much better than a random policy, so it defenitely learns something correctly. I found it very unstable w.r.t. the learning rate and I suspect that a better tuning would require to differentiate between the one of the critic and the one of the actor.\n",
    "\n",
    "Since each training procedure is stochastic, every time the result is different, all other things been equal, so what I'm going to say next is not supported strongly by the data, but can verified if one has time using an ensemble of agents and averaging the performances at each epoch.\n",
    "\n",
    "What I observed is that:\n",
    "- the TD agent is more unstable than the MC one;\n",
    "- shaping the reward function changes a lot the results. I've done this in 2 ways: the first one is to give -1 instead of +1 to the last reward of an episode if the episode ends with the cartpole falling. This enables the agent to differentiate between an episode ended by truncation (good, it scored the maximum possible) or one in which it committed a sequence of non-optimal actions (bad, could have done better). The second one is similar but more sophisticated and is based on the idea of smoothing the reward, so that the responsability for failing the task gets shared in a weighted way by the last actions taken (with polynomial decay, whose power is a parameter of the model). I found that with these 2 changes the A2C with Monte Carlo estimation reached the maximum reward possible and was much more stable on that performance than all other configurations.\n",
    "\n",
    "Anyway there is always a source of instability causing sudden drops in performance during the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
