{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving cartpole task with policy-based RL algorithms\n",
    "\n",
    "**Decription of the task:**\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gym\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(agent, env, return_states=False):\n",
    "    # Reset environment (start of an episode)\n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    done = []\n",
    "    \n",
    "    if return_states:\n",
    "        states = [state]\n",
    "        \n",
    "        \n",
    "    steps = 0\n",
    "    while True:\n",
    "        action, log_prob = agent.get_action(state, return_log = True)\n",
    "        new_state, reward, terminal, info = env.step(action) # gym standard step's output\n",
    "        \n",
    "        if return_states:\n",
    "            states.append(new_state)\n",
    "            \n",
    "        if terminal and 'TimeLimit.truncated' not in info:\n",
    "            # give -1 if cartpole falls but not if episode is truncated\n",
    "            reward = -1 \n",
    "            \n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        done.append(terminal)\n",
    "        \n",
    "        if terminal:\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "       \n",
    "    rewards = np.array(rewards)\n",
    "    done = np.array(done)\n",
    "    \n",
    "    if return_states:\n",
    "        return rewards, log_probs, np.array(states), done\n",
    "    else:\n",
    "        return rewards, log_probs, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_test_episode(agent):\n",
    "    # Create environment\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = agent.get_action(state, return_log = False)\n",
    "        new_state, reward, terminal, info = env.step(action) # gym standard step's output\n",
    "        if terminal: \n",
    "            break\n",
    "        else: \n",
    "            state = new_state\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Policy-Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PolicyGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(PolicyGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_cartpole(n_episodes = 100, lr = 0.01, gamma = 0.99):\n",
    "    # Create environment\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    # Init agent\n",
    "    agent = PolicyGradient.PolicyGrad(observation_space, action_space, lr, gamma)\n",
    "    performance = []\n",
    "    losses = []\n",
    "    for e in range(n_episodes):\n",
    "        rewards, log_probs, _ = play_episode(agent, env)\n",
    "        performance.append(np.sum(rewards))\n",
    "        if (e+1)%10 == 0:\n",
    "            print(\"Episode %d - reward: %.0f\"%(e+1, np.mean(performance[-10:])))\n",
    "        \n",
    "        loss = agent.update(rewards, log_probs)\n",
    "        losses.append(loss)\n",
    "    return agent, np.array(performance), np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trained_agentPG, cumulative_rewardPG, lossesPG = train_cartpole(n_episodes = 500, lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = False\n",
    "if T:\n",
    "    n_runs = 30\n",
    "    results_v0 = []\n",
    "    for i in range(n_runs):\n",
    "        trained_agentPG, cumulative_rewardPG, lossesPG = train_cartpole(n_episodes = 500, lr=5e-3)\n",
    "        results_v0.append(cumulative_rewardPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if T:\n",
    "    np.save('Results/REINFORCE_perf', results_v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1,len(cumulative_rewardPG)+1)\n",
    "plt.plot(episodes, cumulative_rewardPG)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episodes, lossesPG)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_test_episode(trained_agentPG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor-Critic - trajectory version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ActorCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ActorCritic' from '/home/nicola/Nicola_unipd/MasterThesis/Policy-based-RL/ActorCritic.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(ActorCritic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cartpole_A2C(n_epochs = 100, lr = 0.01, gamma = 0.99, TD=True, twin=False, tau=1.,**kwargs):\n",
    "    # Create environment\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    # Init agent\n",
    "    agent = ActorCritic.A2C(observation_space, action_space, lr, gamma, \n",
    "                            TD=TD, discrete=False, twin=twin, tau=tau, **kwargs)\n",
    "    performance = []\n",
    "    score = []\n",
    "    for e in range(n_epochs):\n",
    "        rewards, log_probs, states, done = play_episode(agent, env, return_states=True)\n",
    "        performance.append(np.sum(rewards))\n",
    "        if (e+1)%10 == 0:\n",
    "            print(\"Episode %d - reward: %.0f\"%(e+1, np.mean(performance[-10:])))\n",
    "        #print(\"rewards.shape \", rewards.shape)\n",
    "        #print(\"log_probs \", log_probs)\n",
    "        #print(\"states.shape \", states.shape)\n",
    "        #print(\"done.shape \", done.shape)\n",
    "        #print(\"done \", done)\n",
    "        agent.update(rewards, log_probs, np.array([states]) , done)\n",
    "        \n",
    "    return agent, np.array(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== A2C HyperParameters ==========\n",
      "Discount factor:  0.99\n",
      "Learning rate:  0.001\n",
      "Action space:  2\n",
      "Discrete state space:  False\n",
      "Temporal Difference learning:  True\n",
      "Twin networks:  True\n",
      "Update critic target factor:  0.1\n",
      "Device used:  cpu\n",
      "\n",
      "\n",
      "========== A2C Architecture ==========\n",
      "Actor architecture: \n",
      " Actor(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=16, out_features=2, bias=True)\n",
      "    (7): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "Critic architecture: \n",
      " Critic(\n",
      "  (net1): BasicCritic(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=16, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (net2): BasicCritic(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=16, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Critic target architecture: \n",
      "Critic(\n",
      "  (net1): BasicCritic(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=16, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (net2): BasicCritic(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=16, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Episode 10 - reward: 12\n",
      "Episode 20 - reward: 12\n",
      "Episode 30 - reward: 17\n",
      "Episode 40 - reward: 12\n",
      "Episode 50 - reward: 12\n",
      "Episode 60 - reward: 17\n",
      "Episode 70 - reward: 12\n",
      "Episode 80 - reward: 15\n",
      "Episode 90 - reward: 15\n",
      "Episode 100 - reward: 12\n",
      "Episode 110 - reward: 12\n",
      "Episode 120 - reward: 12\n",
      "Episode 130 - reward: 14\n",
      "Episode 140 - reward: 18\n",
      "Episode 150 - reward: 14\n",
      "Episode 160 - reward: 19\n",
      "Episode 170 - reward: 18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d0414d312173>\u001b[0m in \u001b[0;36mtrain_cartpole_A2C\u001b[0;34m(n_epochs, lr, gamma, TD, twin, tau, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(\"done.shape \", done.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(\"done \", done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/Policy-based-RL/ActorCritic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_TD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_MC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/Policy-based-RL/ActorCritic.py\u001b[0m in \u001b[0;36mupdate_TD\u001b[0;34m(self, rewards, log_probs, states, done)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_critic_TD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_actor_TD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/Policy-based-RL/ActorCritic.py\u001b[0m in \u001b[0;36mupdate_actor_TD\u001b[0;34m(self, rewards, log_probs, new_states, old_states, done)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mpolicy_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "HPs = dict(n_epochs=5000, lr=1e-3, twin=True,tau=0.1, debug=True, hiddens=[64,32,16])\n",
    "agent_TD, performance_TD = train_cartpole_A2C(**HPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1,len(performance_TD)+1)\n",
    "plt.scatter(episodes, performance_TD, s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_test_episode(agent_TD) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "agent_MC, performance_MC = train_cartpole_A2C(n_epochs=1500, lr=5e-3, TD=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1,len(performance_MC)+1)\n",
    "plt.scatter(episodes, performance_MC, s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_test_episode(agent_MC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward shaping\n",
    "\n",
    "Try to make a more informative reward.\n",
    "Idea: store the whole trajectory, then subtract $-\\frac{eps \\cdot t}{T}$ to all rewards, where $t$ is the step at which the reward was obtained and $T$ the total number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_rewards(r, eps, power=1):\n",
    "    T = len(r)\n",
    "    t = np.arange(1,T+1)\n",
    "    r -= eps*(t/T)**power\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cartpole_A2C_shaped(n_epochs = 100, n_batches = 1, lr = 0.01, gamma = 0.99, TD=True, eps=1, power=1):\n",
    "    # Create environment\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    # Init agent\n",
    "    agent = ActorCritic.A2C(observation_space, action_space, lr, gamma, TD=TD, discrete=False)\n",
    "    performance = []\n",
    "    for e in range(n_epochs):\n",
    "        r_list = []\n",
    "        logp_list = []\n",
    "        s_list = []\n",
    "        done_list = []\n",
    "        score = []\n",
    "        \n",
    "        for b in range(n_batches):\n",
    "            rewards, log_probs, states, done = play_episode(agent, env, return_states=True)\n",
    "            if done[-1] == True and len(done) != 500:\n",
    "                rewards = shape_rewards(rewards, eps, power)\n",
    "            r_list.append(rewards)\n",
    "            logp_list.append(log_probs)\n",
    "            s_list.append(states)\n",
    "            done_list.append(done)\n",
    "            score.append(np.sum(rewards))\n",
    "            \n",
    "        performance.append(np.mean(score))\n",
    "        if (e+1)%10 == 0:\n",
    "            print(\"Episode %d - reward: %.0f\"%(e+1, np.mean(performance[-10:])))\n",
    "        exp_buff = experience_buffer(r_list, logp_list, s_list, done_list)\n",
    "        rewards, log_probs, states, done = exp_buff.get_exp()\n",
    "        #print(\"rewards.shape \", rewards.shape)\n",
    "        #print(\"log_probs \", log_probs)\n",
    "        #print(\"states.shape \", states.shape)\n",
    "        #print(\"done.shape \", done.shape)\n",
    "        #print(\"done \", done)\n",
    "        agent.update(rewards, log_probs[0], states, done)\n",
    "        \n",
    "    return agent, np.array(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "agent_TD_sh, performance_TD_sh = train_cartpole_A2C_shaped(n_epochs=1500, lr=5e-3, power=2, eps=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1,len(performance_TD_sh)+1)\n",
    "plt.scatter(episodes, performance_TD_sh, s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_test_episode(agent_TD_sh) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "agent_MC_sh, performance_MC_sh = train_cartpole_A2C_shaped(n_epochs=1500, lr=5e-3, power=2, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1,len(performance_MC_sh)+1)\n",
    "plt.scatter(episodes, performance_MC_sh, s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_test_episode(agent_MC_sh) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final discussion:** It is undisputable that A2C setup is much better than a random policy, so it defenitely learns something correctly. I found it very unstable w.r.t. the learning rate and I suspect that a better tuning would require to differentiate between the one of the critic and the one of the actor.\n",
    "\n",
    "Since each training procedure is stochastic, every time the result is different, all other things been equal, so what I'm going to say next is not supported strongly by the data, but can verified if one has time using an ensemble of agents and averaging the performances at each epoch.\n",
    "\n",
    "What I observed is that:\n",
    "- the TD agent is more unstable than the MC one;\n",
    "- shaping the reward function changes a lot the results. I've done this in 2 ways: the first one is to give -1 instead of +1 to the last reward of an episode if the episode ends with the cartpole falling. This enables the agent to differentiate between an episode ended by truncation (good, it scored the maximum possible) or one in which it committed a sequence of non-optimal actions (bad, could have done better). The second one is similar but more sophisticated and is based on the idea of smoothing the reward, so that the responsability for failing the task gets shared in a weighted way by the last actions taken (with polynomial decay, whose power is a parameter of the model). I found that with these 2 changes the A2C with Monte Carlo estimation reached the maximum reward possible and was much more stable on that performance than all other configurations.\n",
    "\n",
    "Anyway there is always a source of instability causing sudden drops in performance during the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
